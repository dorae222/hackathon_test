import os
from typing import List, Optional
from fastapi import FastAPI, HTTPException, Path
from pydantic import BaseModel
from openai import OpenAI
import httpx

app = FastAPI(title="AI Chat Service")

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    system_prompt: Optional[str] = None
    messages: List[ChatMessage]
    model: Optional[str] = None


class ChatByAnimalRequest(BaseModel):
    messages: List[ChatMessage]
    model: Optional[str] = None

def _build_client_and_model(model: Optional[str]):
    """
    Returns (client, model_name, is_fake)
    - If OPENAI_API_KEY is missing, returns (None, "local-fake", True)
    - Otherwise returns real OpenAI client.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        # Dev-friendly fallback to keep UI working without external key
        return None, "local-fake", True
    client = OpenAI(api_key=api_key)
    return client, (model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")), False

def _vision_base_url() -> str:
    # When running in docker-compose, the service name resolves in the bridge network
    return os.getenv("AI_VISION_BASE_URL", "http://ai-vision:8000")

def _extract_field(info_list, key: str) -> str:
    if not info_list:
        return ""
    # info is like [[k,v,k2,v2], ...]
    for row in info_list:
        if not isinstance(row, list):
            continue
        if len(row) >= 2 and row[0] == key:
            return row[1] or ""
        if len(row) >= 4 and row[2] == key:
            return row[3] or ""
    return ""


def _fake_llm_reply(messages: List[dict], system_prompt: Optional[str] = None) -> str:
    """Very small local stub to simulate an assistant in dev.
    Echoes the last user message with a friendly Korean persona.
    """
    user_text = ""
    for m in reversed(messages):
        if m.get("role") == "user":
            user_text = m.get("content", "")
            break
    intro = "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°€ì›Œìš”! ì €ëŠ” ì„ì‹œ ì±—ë´‡ì´ì—ìš”(ì˜¤í”„ë¼ì¸ ëª¨ë“œ). "
    if system_prompt:
        intro += "(í”„ë¡œí•„ ê¸°ë°˜ ë‹µë³€ì„ í‰ë‚´ ë‚´ëŠ” ì¤‘) "
    if not user_text:
        return intro + "ë¬´ì—‡ì´ë“  í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”. ë©! ğŸ¶"
    return (
        f"{intro}ë°©ê¸ˆ ì´ë ‡ê²Œ ë§ì”€í•˜ì…¨ì–´ìš”: '{user_text}'. "
        "ì§€ê¸ˆì€ ê°„ë‹¨í•œ ë°ëª¨ ëª¨ë“œë¼ ìƒì„¸í•œ ë‹µë³€ì€ ì–´ë µì§€ë§Œ, í•„ìš”í•œ ì£¼ì œë¥¼ ë” ì•Œë ¤ì£¼ì‹œë©´ ìµœëŒ€í•œ ë„ì™€ë³¼ê²Œìš”! ğŸ¾"
    )

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/chat")
async def chat(req: ChatRequest):
    try:
        client, model_name, is_fake = _build_client_and_model(req.model)
        msgs = []
        if req.system_prompt:
            msgs.append({"role":"system","content":req.system_prompt})
        msgs.extend([m.model_dump() for m in req.messages])
        if is_fake:
            reply = _fake_llm_reply(msgs, req.system_prompt)
        else:
            resp = client.chat.completions.create(model=model_name, messages=msgs)
            reply = resp.choices[0].message.content or ""
        return {"reply": reply}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat failed: {e}")


@app.post("/chat/animal/{animal_id}")
async def chat_by_animal(animal_id: int = Path(..., ge=0), req: ChatByAnimalRequest = None):
    """
    Chat endpoint that builds the system prompt on the server using
    the animal profile fetched from ai-vision. Frontend should not send
    any system prompt.
    """
    try:
        # 1) Fetch animal profile
        async with httpx.AsyncClient(timeout=10.0) as client_http:
            resp = await client_http.get(f"{_vision_base_url()}/animals/{animal_id}")
            resp.raise_for_status()
            animal = resp.json()

        info = animal.get("info", [])
        dog_name = _extract_field(info, "ì´ë¦„") or "ì´ë¦„ ë¯¸ìƒ"
        breed = _extract_field(info, "í’ˆì¢…") or "-"
        age = _extract_field(info, "ë‚˜ì´(ì¶”ì •)") or "-"
        gender = _extract_field(info, "ì„±ë³„") or "-"
        weight = _extract_field(info, "ë¬´ê²Œ") or "-"
        color = _extract_field(info, "í„¸ìƒ‰") or "-"
        status = _extract_field(info, "ìƒíƒœ") or "-"
        serial = _extract_field(info, "ì¼ë ¨ë²ˆí˜¸") or "-"
        notes = _extract_field(info, "íŠ¹ì´ì‚¬í•­") or "-"

        profile_lines = "\n".join([
            f"ì´ë¦„: {dog_name}",
            f"í’ˆì¢…: {breed}",
            f"ë‚˜ì´(ì¶”ì •): {age}",
            f"ì„±ë³„: {gender}",
            f"ë¬´ê²Œ: {weight}",
            f"í„¸ìƒ‰: {color}",
            f"ìƒíƒœ: {status}",
            f"ì¼ë ¨ë²ˆí˜¸: {serial}",
            f"íŠ¹ì´ì‚¬í•­: {notes}",
        ])

        region = os.getenv("SERVICE_REGION", "ê´‘ì£¼")
        preferred_domains = os.getenv("PREFERRED_DOMAINS", "kcanimal.or.kr")

        system_prompt = "\n".join([
            f"ë‹¹ì‹ ì€ {region}ì‹œ ìœ ê¸°ê²¬ ë³´í˜¸ì†Œì˜ ë°˜ë ¤ê²¬ ì±—ë´‡ì…ë‹ˆë‹¤.",
            "ì•„ë˜ í”„ë¡œí•„ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì •ì¤‘í•œ í•œêµ­ì–´ë¡œ ë‹µí•˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.",
            "ê°€ëŠ¥í•˜ë©´ ì¹œê·¼í•œ ê°•ì•„ì§€ ë§íˆ¬(ë©ë©, ì™ˆì™ˆ)ë¥¼ ì‚¬ìš©í•˜ë˜ ê³¼í•˜ì§€ ì•Šê²Œ í•´ì£¼ì„¸ìš”.",
            f"ë§í¬ë‚˜ ì°¸ê³ ìë£Œë¥¼ ì–¸ê¸‰í•  ê²½ìš° ë‹¤ìŒ ë„ë©”ì¸ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”: {preferred_domains}",
            "í”„ë¡œí•„ì— ì—†ëŠ” ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.",
            "ëŒ€í™”ê°€ ì¶©ë¶„íˆ ì§„í–‰ë˜ì–´ ì‚¬ìš©ìê°€ ì¶”ì²œì„œë¥¼ ìš”ì²­í•˜ê±°ë‚˜ í•©ì˜í•˜ë©´, ë°˜ë“œì‹œ 'ì¶”ì²œì„œ:'ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ë½ í•˜ë‚˜ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.",
            "ê·¸ ë‹¨ë½ì—ëŠ” ì…ì–‘ì ë¼ì´í”„ìŠ¤íƒ€ì¼ ìš”ì•½(ëŒ€í™”ì—ì„œ ì¶”ì¶œ), ë°˜ë ¤ê²¬ì˜ ê°•ì , ì£¼ì˜ì‚¬í•­, ì²« ì¼ì£¼ì¼ ì ì‘ íŒì„ ê°„ê²°íˆ í¬í•¨í•˜ì„¸ìš”.",
            "",
            "[ë°˜ë ¤ê²¬ í”„ë¡œí•„]",
            profile_lines,
        ])

        # 2) Call OpenAI
        client, model_name, is_fake = _build_client_and_model(req.model if req else None)
        msgs = [{"role": "system", "content": system_prompt}]
        if req and req.messages:
            msgs.extend([m.model_dump() for m in req.messages])

        if is_fake:
            reply = _fake_llm_reply(msgs, system_prompt)
        else:
            resp = client.chat.completions.create(model=model_name, messages=msgs)
            reply = resp.choices[0].message.content or ""
        return {"reply": reply}
    except httpx.HTTPError as he:
        raise HTTPException(status_code=502, detail=f"Failed to fetch animal profile: {he}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat (animal) failed: {e}")